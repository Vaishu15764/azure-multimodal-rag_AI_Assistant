# ğŸ“„ Azure MultiModel RAG AI Assistant
**Azure Blob Storage | Pinecone | Groq LLaMA | Gemini | FastAPI | Advanced UI**

---

## ğŸ” Project Overview
This project implements a **Multimodal Retrieval-Augmented Generation (RAG) system** capable of answering user queries from **complex PDF documents** containing:

- ğŸ“„ Text  
- ğŸ“Š Tables  
- ğŸ“ Mathematical formulas  
- ğŸ–¼ï¸ Images & diagrams  

The system ingests PDFs from **Azure Blob Storage**, extracts and processes all modalities, converts them into semantic embeddings, stores them in **Pinecone**, and generates **accurate, context-aware answers** using **Groq LLaMA-3.3-70B**.

A **FastAPI backend** powers an **enterprise-style web UI** that supports:
- Multi-turn conversation memory
- Voice input (Speech-to-Text)
- Voice output (Text-to-Speech)
- Source viewing
- Chat history management

---
## ğŸ–¥ï¸ UI Screenshots

### ğŸ”¹ Main Chat Interface
<img width="1626" height="886" alt="image" src="https://github.com/user-attachments/assets/42f4e8bd-b663-48e6-aaa3-8116bff0f85b" />

### ğŸ”¹ Multimodal Answer with Sources
<img width="743" height="833" alt="image" src="https://github.com/user-attachments/assets/64121de5-7f60-4e78-bef6-377aa84dcf1e" />

## ğŸ¯ Problem Statement
Traditional document Q&A systems:
- Fail to understand tables, formulas, and images
- Struggle with large PDFs
- Lose context during follow-up questions
- Produce hallucinated answers
- Provide poor user experience

This project solves these problems by building a **scalable, multimodal, memory-enabled RAG system with a production-style UI**, similar to real-world **enterprise document intelligence platforms**.

---

## âœ… Key Features

### ğŸ”¹ Multimodal Intelligence
- Text extraction & chunking
- Table extraction
- Formula extraction
- Image extraction + captioning (Gemini)

### ğŸ”¹ RAG & AI
- Local embeddings (Sentence-Transformers)
- Pinecone semantic vector search
- Groq LLaMA-3.3-70B for fast inference
- Strict context-grounded answers
- Multi-turn conversation memory

### ğŸ”¹ Enterprise UI (index.html)
- Clean chat interface
- Sidebar with chat history
- View document sources per answer
- Typing indicator
- ğŸ¤ Voice input (Speech-to-Text)
- ğŸ”Š Voice output (Text-to-Speech)
- Reset chat / clear memory

### ğŸ”¹ Backend
- FastAPI-based REST API
- Modular & scalable architecture
- Ready for cloud deployment

---

## ğŸ—ï¸ High-Level Architecture

Azure Blob Storage (PDF)
â†“
Multimodal Extraction
(Text | Tables | Formulas | Images)
â†“
Chunking & Captioning
â†“
Local Embeddings (Sentence-Transformer)
â†“
Pinecone Vector Database
â†“
RAG Retrieval
â†“
Groq LLaMA-3.3-70B
â†“
FastAPI Backend
â†“
Enterprise Web UI (HTML + JS)


---

## ğŸ§© Module-wise Explanation

### 1ï¸âƒ£ Data Ingestion (`read_data.py`)
- Downloads PDF documents from **Azure Blob Storage**
- Reads files as byte streams
- Entry point of the indexing pipeline

---

### 2ï¸âƒ£ Multimodal Extraction
- **Text** â†’ Extracted & chunked (`chunking.py`)
- **Tables** â†’ Extracted as structured text (`table.py`)
- **Formulas** â†’ Extracted as mathematical expressions (`formula.py`)
- **Images** â†’ Extracted and captioned using **Gemini API** (`img.py`)

Extracted outputs are stored in:
- `output_tables/`
- `output_formulas/`
- `output_images/`

---

### 3ï¸âƒ£ Embedding Generation (`embedding.py`)
- Uses **Sentence-Transformers (all-MiniLM-L6-v2)**
- Fully local & free embedding generation
- Batch processing for performance

---

### 4ï¸âƒ£ Vector Storage (`vector_store.py`)
- Uses **Pinecone Serverless**
- Cosine similarity-based search
- Rich metadata support:
  - `type` (text / table / formula / image)
  - `source`
  - `image_path`
- Batch upserts for efficiency

---

### 5ï¸âƒ£ RAG & LLM Logic (`chat_app.py`)
- Converts user query to embeddings
- Retrieves top-K relevant chunks from Pinecone
- Generates grounded answers using **Groq LLaMA-3.3-70B**
- Maintains **conversation memory** for follow-up questions
- CLI chat mode for testing

---

### 6ï¸âƒ£ Pipeline Orchestration (`main.py`)
- Runs the **complete indexing pipeline**
- Connects:
  - Download â†’ Extract â†’ Embed â†’ Store
- Executed once per document ingestion

---

### 7ï¸âƒ£ API Layer (`fast_api.py`)
- Built using **FastAPI**
- API Endpoints:
  - `POST /chat` â†’ Ask questions
  - `POST /reset-chat` â†’ Clear conversation memory
- Serves `index.html` as the UI
- Production-ready backend design

---

### 8ï¸âƒ£ Frontend UI (`index.html`)
- Enterprise-style chat interface
- Sidebar with chat history
- Source viewer per response
- Typing animation
- ğŸ¤ Speech-to-Text input
- ğŸ”Š Text-to-Speech output
- Local session persistence (browser storage)

---

## ğŸ“‚ Project Folder Structure

Multimodal_RAG_Project/
â”‚
â”œâ”€â”€ __pycache__/
â”‚
â”œâ”€â”€ output_formulas/          # Extracted mathematical formulas
â”œâ”€â”€ output_images/            # Extracted images & captions
â”œâ”€â”€ output_tables/            # Extracted tables
â”‚
â”œâ”€â”€ rag_env/                  # Python virtual environment
â”‚
â”œâ”€â”€ static/                   # Static files (CSS / JS if extended)
â”‚
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ requirements.txt          # Project dependencies
â”‚
â”œâ”€â”€ app.py                    # (Optional) App entry / experimentation
â”œâ”€â”€ main.py                   # Full multimodal indexing pipeline
â”œâ”€â”€ fast_api.py               # FastAPI backend server
â”œâ”€â”€ chat_app.py               # RAG logic + Groq LLM + memory
â”‚
â”œâ”€â”€ read_data.py              # Azure Blob Storage PDF ingestion
â”œâ”€â”€ chunking.py               # Text extraction & chunking
â”œâ”€â”€ embedding.py              # Local embedding generation
â”œâ”€â”€ vector_store.py           # Pinecone vector DB integration
â”‚
â”œâ”€â”€ img.py                    # Image extraction & Gemini captioning
â”œâ”€â”€ table.py                  # Table extraction logic
â”œâ”€â”€ formula.py                # Formula extraction logic
â”‚
â”œâ”€â”€ index.html                # Enterprise-style chat UI
â”‚
â””â”€â”€ README.md                 # Project documentation


---

## âš™ï¸ Tech Stack
- **Language**: Python  
- **Backend**: FastAPI  
- **Storage**: Azure Blob Storage  
- **Vector Database**: Pinecone (Serverless)  
- **Embeddings**: Sentence-Transformers  
- **LLM**: Groq LLaMA-3.3-70B  
- **Vision Model**: Google Gemini  
- **Frontend**: HTML, CSS, JavaScript  
- **Libraries**: LangChain, PyPDF2  

---

## ğŸ”‘ Environment Variables (`.env`)
```env
connection_url=AZURE_CONNECTION_STRING
container_name=AZURE_CONTAINER_NAME
blob_name=PDF_NAME.pdf

PINECONE_API_KEY=YOUR_PINECONE_KEY
PINECONE_INDEX_NAME=multimodal-rag-index

GROQ_API_KEY=YOUR_GROQ_KEY
Gemini_Api=YOUR_GEMINI_KEY
```  
---
## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```  

### 2ï¸âƒ£ Run Indexing Pipeline (One Time)
```bash
python main.py
```

### 3ï¸âƒ£ Start FastAPI Server
```bash
python fast_api.py
```

### ğŸŒ Open in Browser
```bash
http://localhost:8000
```  

## ğŸ§ª Example Use Case

**User:**  
â€œExplain the attention formula shown in the diagram.â€

**System:**  
- Retrieves related text, formulas, and image captions  
- Generates a context-grounded answer  
- Allows voice playback of the response  
- Maintains memory for follow-up questions  

---

## ğŸš€ Future Enhancements
- PDF upload via UI  
- Highlight text sources in documents  
- Image rendering inside chat  
- Authentication and role-based access  
- Cloud deployment (Azure / AWS)  

---

## ğŸ“Œ Conclusion
This project demonstrates a **practical and scalable Multimodal RAG-based document intelligence system** with an **enterprise-grade UI**, capable of answering questions over complex PDFs containing both textual and visual information. By combining efficient retrieval, grounded generation, multimodal understanding, and a rich user interface, the system closely mirrors **real-world enterprise AI document solutions**.


---

## ğŸ‘¤ Author

**Vaishnavi Sainath Pachange**  
ğŸ“ B.Tech (CSE) | Data Science & AI Enthusiast  
ğŸ’¡ Interested in AI, Machine Learning, Generative AI & Intelligent Systems  


---




